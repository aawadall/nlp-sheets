{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm6UT-3OL2C8"
      },
      "source": [
        "# NLP Experiment 1: Word Embeddings\n",
        "Experimenting With NLP using NLTK and Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFUpKXZL8I2"
      },
      "source": [
        "## Load Required Libraries\n",
        "Load libraries expected for this experiment:\n",
        "\n",
        "- [ ] nltk \n",
        "- [ ] nltk.word_tokenize \n",
        "- [ ] spacy.lang.en.English \n",
        "- [ ] gensim \n",
        "- [ ] numpy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in /home/ahmed/.local/lib/python3.10/site-packages (3.7)\n",
            "Requirement already satisfied: joblib in /home/ahmed/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/ahmed/.local/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /home/ahmed/.local/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /home/ahmed/.local/lib/python3.10/site-packages (from nltk) (7.1.2)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in /home/ahmed/.local/lib/python3.10/site-packages (3.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (1.23.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (0.10.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ahmed/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/ahmed/.local/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ahmed/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ahmed/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ahmed/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ahmed/.local/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ahmed/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Local only \n",
        "!pip install --upgrade nltk\n",
        "!pip install --upgrade spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SKF750bGMsAw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahmed/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-11-30 20:31:05.643633: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-11-30 20:31:05.643651: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-11-30 20:31:07.325834: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-11-30 20:31:07.325852: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-11-30 20:31:07.325865: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aawadall): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize \n",
        "from spacy.lang.en import English \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ1IWUONNCog",
        "outputId": "a02ceb38-6dee-4a4c-f32e-bc6b34fca415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gensim in /home/ahmed/.local/lib/python3.10/site-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/ahmed/.local/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/ahmed/.local/lib/python3.10/site-packages (from gensim) (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/ahmed/.local/lib/python3.10/site-packages (from gensim) (1.23.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QNuXdyEZNIxX"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api \n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtuwFbKfNRJN"
      },
      "source": [
        "Download word2vec, this may take some time "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "5gtUk2djNV1S",
        "outputId": "b641f2c6-0770-45bb-bfed-8caf23b424c0"
      },
      "outputs": [],
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuQiR4m_NjEf"
      },
      "source": [
        "## Define Sample Text\n",
        "define a couple of sample text strings and run some analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NktwZiktNseA"
      },
      "outputs": [],
      "source": [
        "simple_str = \"This is a simple string, used to test tokenization. It may have minimal information in it, but used to illustrate how to extract word embeddings\"\n",
        "strings_vector = [\n",
        "    \"Patiance is a virtue\",\n",
        "    \"The sum of one and two is \",\n",
        "    \"better safe than sorry\",\n",
        "    \"you cannot divide a number by zero\",\n",
        "    \"are we there yet?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk_mRDtlPcvE"
      },
      "source": [
        "## Sentence Concept \n",
        "Define a class of functions to extract the gist of a sentence using an aggregate of word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rQQnmmXLP23w"
      },
      "outputs": [],
      "source": [
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer\n",
        "def normalized_sum(sentence):\n",
        "  \"\"\"\n",
        "  Calculates normalized sum of word embeddings of tokens in a sentence\n",
        "  \"\"\"\n",
        "\n",
        "  toks = tokenizer(sentence)\n",
        "  sum = []\n",
        "  for token in toks:\n",
        "    try: \n",
        "      v = wv[token.text]\n",
        "      if len(sum) == 0:\n",
        "        sum = v\n",
        "      else:\n",
        "        sum = sum + v \n",
        "    except:\n",
        "      continue\n",
        "  # normalize \n",
        "  sum = sum / np.linalg.norm(sum)\n",
        "  return sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTmKMKi6Rvts"
      },
      "source": [
        "## Calculate Normalized Sum \n",
        "Calculate normalized sum for sample sentences "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2f2huOAMR4KO"
      },
      "outputs": [],
      "source": [
        "simple_str_ns = normalized_sum(simple_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Simple String Embedding\n",
        "find closest words matching this concept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('By_Laurelle_Gilbert', 0.6034723520278931),\n",
              " ('%_#F########_9v.jsn', 0.5943512916564941),\n",
              " ('BY_ANDY_THOMPSON', 0.589264988899231),\n",
              " ('By_Jonas_Elmerraji', 0.585291862487793),\n",
              " ('it', 0.585139811038971),\n",
              " ('A.It_s', 0.5839791297912598),\n",
              " ('that', 0.5821940898895264),\n",
              " ('AThere', 0.5760250687599182),\n",
              " ('but', 0.5712571740150452),\n",
              " ('GREG_POTTER_Yes', 0.5703470706939697)]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar(simple_str_ns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try for other sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patiance is a virtue\n",
            "[('virtue', 0.836878776550293), ('is', 0.6671273112297058), ('means', 0.578962504863739), ('presupposes', 0.5349286794662476), ('dint', 0.5220244526863098), ('becomes', 0.4905795454978943), (\"isn'ta\", 0.48826590180397034), ('seems', 0.48623791337013245), ('dictates', 0.481949120759964), ('virture', 0.47703272104263306)]\n",
            "The sum of one and two is \n",
            "[('one', 0.673549473285675), ('sum', 0.628027081489563), ('two', 0.5564963817596436), ('three', 0.5492445230484009), ('the', 0.5443174242973328), ('four', 0.539329469203949), ('five', 0.532472550868988), ('is', 0.5301937460899353), ('The', 0.5249226689338684), ('six', 0.5138891935348511)]\n",
            "better safe than sorry\n",
            "[('better', 0.7288674712181091), ('sorry', 0.6563988924026489), ('safe', 0.6418575048446655), ('safer', 0.6321291327476501), ('sorry_Semaitis', 0.618122935295105), ('happier', 0.5762538313865662), ('nicer', 0.5709905624389648), ('TV6_wonders', 0.5573280453681946), ('sorrier', 0.5523179173469543), ('Johnnie_Baston_die', 0.5482122898101807)]\n",
            "you cannot divide a number by zero\n",
            "[('can', 0.6353193521499634), ('not', 0.6165188550949097), ('you', 0.6003228425979614), ('do', 0.5930917263031006), ('they', 0.5682259202003479), ('AWell', 0.5661731958389282), ('just', 0.5656866431236267), ('QDo', 0.5572266578674316), ('actually', 0.554905891418457), ('that', 0.5464920401573181)]\n",
            "are we there yet?\n",
            "[('we', 0.7336916923522949), ('there', 0.7173899412155151), ('they', 0.6653739809989929), (\"'re\", 0.6484829783439636), ('are', 0.6478847861289978), ('not', 0.6304683685302734), ('yet', 0.6262755990028381), ('have', 0.6164054274559021), ('still', 0.6158721446990967), ('do', 0.6152428388595581)]\n"
          ]
        }
      ],
      "source": [
        "for s in strings_vector:\n",
        "  print(s)\n",
        "  print(wv.most_similar(normalized_sum(s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dot Product\n",
        "find the dot product of all word embeddings in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(sentence):\n",
        "    \"\"\"\n",
        "    Define a function to calculate the \n",
        "    resulting cosine similarity between\n",
        "    all word embeddings in a sentence\n",
        "    \"\"\"\n",
        "    toks = tokenizer(sentence)\n",
        "    sim = None\n",
        "    for token in toks:\n",
        "        try:\n",
        "            v = wv[token.text]\n",
        "            if sim == None:\n",
        "                sim = v \n",
        "                print('first', sim)\n",
        "            else:\n",
        "                sim = np.linalg.multi_dot(sim, v)\n",
        "                print(sim)\n",
        "        except:\n",
        "            continue\n",
        "    return sim / np.linalg.norm(sim)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a simple string, used to test tokenization. It may have minimal information in it, but used to illustrate how to extract word embeddings\n",
            "first [-0.2890625   0.19921875  0.16015625  0.02526855 -0.23632812  0.10205078\n",
            "  0.06640625 -0.16503906  0.12597656  0.22070312  0.05517578 -0.28710938\n",
            " -0.02148438  0.05541992  0.01574707  0.29296875  0.19433594 -0.01531982\n",
            "  0.03955078 -0.21484375  0.00994873  0.16015625  0.07958984 -0.05932617\n",
            "  0.12353516 -0.27148438 -0.10205078  0.078125   -0.07519531  0.22363281\n",
            "  0.16210938 -0.04614258  0.12304688  0.07275391  0.25        0.0072937\n",
            " -0.38867188  0.10644531  0.20996094  0.06103516  0.10107422  0.16894531\n",
            " -0.15429688 -0.08251953  0.06542969 -0.12255859 -0.11621094  0.04248047\n",
            "  0.08251953  0.09716797 -0.05371094  0.125       0.15039062 -0.09228516\n",
            "  0.23925781  0.15234375  0.1796875  -0.26171875  0.15429688  0.09619141\n",
            " -0.30859375 -0.05224609 -0.18652344 -0.24414062 -0.0612793  -0.12695312\n",
            "  0.14160156 -0.03295898  0.03759766 -0.09863281  0.07324219 -0.046875\n",
            "  0.08203125  0.02441406  0.11425781  0.05200195  0.02685547  0.04931641\n",
            "  0.11035156  0.44921875  0.07519531 -0.06835938 -0.10351562  0.06591797\n",
            " -0.15234375 -0.21484375 -0.09179688  0.37109375 -0.07470703 -0.08886719\n",
            "  0.12255859 -0.12792969 -0.12597656 -0.23632812  0.06225586  0.05957031\n",
            "  0.08251953  0.05297852 -0.17871094  0.21289062  0.00241089 -0.05786133\n",
            "  0.02563477 -0.01434326  0.00646973 -0.0703125  -0.08398438 -0.18652344\n",
            " -0.24707031 -0.21484375 -0.01269531 -0.02416992  0.14746094  0.20214844\n",
            "  0.19140625  0.06738281  0.02307129  0.05297852 -0.02258301  0.30664062\n",
            " -0.10791016 -0.22460938 -0.10498047  0.06884766  0.07226562  0.07373047\n",
            " -0.11474609 -0.11474609  0.05883789 -0.12109375 -0.15136719  0.04101562\n",
            " -0.09863281  0.06982422  0.32617188 -0.27929688 -0.12255859 -0.12890625\n",
            " -0.12890625  0.19042969  0.171875   -0.05371094 -0.2734375  -0.08496094\n",
            " -0.12695312 -0.24121094  0.09082031 -0.14355469 -0.0703125   0.06396484\n",
            "  0.08007812 -0.20605469  0.03930664  0.04589844  0.0612793  -0.23339844\n",
            "  0.31835938 -0.04956055 -0.03417969 -0.31445312 -0.18066406  0.00842285\n",
            " -0.00765991  0.09423828  0.11865234  0.02722168  0.08398438  0.07128906\n",
            " -0.23339844  0.21777344 -0.17480469  0.05517578  0.08642578 -0.14746094\n",
            " -0.03833008  0.18359375  0.21484375 -0.28125     0.04980469 -0.20800781\n",
            " -0.01177979 -0.08007812 -0.09521484  0.09375    -0.01721191  0.16601562\n",
            " -0.06787109  0.18261719  0.26757812  0.08056641 -0.01373291 -0.16601562\n",
            "  0.13867188 -0.18847656 -0.07666016 -0.29101562 -0.16015625  0.32617188\n",
            " -0.04296875 -0.22558594  0.02868652 -0.25195312  0.08935547  0.203125\n",
            " -0.09375     0.07763672  0.00741577  0.19726562  0.14648438 -0.02148438\n",
            "  0.01611328  0.34570312  0.02990723  0.15625     0.08984375  0.1875\n",
            "  0.06787109  0.04785156 -0.03417969  0.10546875  0.12402344 -0.07177734\n",
            "  0.05981445 -0.03491211  0.10400391 -0.12353516  0.07470703  0.04541016\n",
            "  0.15625     0.02734375  0.00386047 -0.05444336  0.16015625  0.08447266\n",
            " -0.1640625   0.03833008 -0.20800781  0.16015625  0.00065994  0.26367188\n",
            "  0.08496094 -0.23339844 -0.11914062 -0.06494141  0.04736328  0.04980469\n",
            "  0.14941406  0.02099609  0.06201172 -0.16894531  0.171875    0.11865234\n",
            " -0.1875      0.16796875  0.08105469 -0.10595703  0.27539062 -0.20507812\n",
            " -0.06884766 -0.10839844 -0.04467773 -0.00872803 -0.06689453 -0.16308594\n",
            " -0.109375    0.17089844  0.18066406 -0.10351562 -0.265625   -0.03442383\n",
            " -0.06445312  0.28320312  0.13867188  0.10400391  0.10498047 -0.10791016\n",
            "  0.34960938 -0.12158203  0.10644531 -0.11425781 -0.07568359  0.03491211\n",
            "  0.15332031  0.26367188  0.11279297 -0.08349609 -0.15625    -0.203125\n",
            " -0.20605469  0.0255127  -0.2265625  -0.02478027 -0.13378906  0.04907227\n",
            " -0.1328125  -0.27539062  0.00463867  0.12792969  0.12109375 -0.22949219]\n",
            "[-0.11104227  0.07652913  0.06152342  0.00970682 -0.09078456  0.03920242\n",
            "  0.02550971 -0.06339914  0.04839342  0.08478227  0.02119557 -0.11029198\n",
            " -0.00825314  0.02128935  0.00604918  0.11254284  0.07465342 -0.00588505\n",
            "  0.01519328 -0.08253141  0.00382177  0.06152342  0.03057414 -0.02278993\n",
            "  0.04745556 -0.1042897  -0.03920242  0.03001142 -0.028886    0.0859077\n",
            "  0.0622737  -0.0177255   0.04726799  0.02794814  0.09603655  0.00280185\n",
            " -0.14930683  0.04089056  0.0806557   0.02344642  0.03882728  0.06489971\n",
            " -0.05927256 -0.03169957  0.02513457 -0.04708042 -0.04464199  0.01631871\n",
            "  0.03169957  0.03732671 -0.02063285  0.04801828  0.05777199 -0.03545099\n",
            "  0.09190998  0.05852228  0.06902628 -0.10053827  0.05927256  0.03695156\n",
            " -0.11854512 -0.02007014 -0.07165227 -0.0937857  -0.02354021 -0.04876857\n",
            "  0.05439571 -0.01266107  0.014443   -0.03788942  0.02813571 -0.01800685\n",
            "  0.031512    0.00937857  0.04389171  0.01997635  0.01031643  0.01894471\n",
            "  0.04239114  0.17256568  0.028886   -0.02626    -0.03976514  0.02532214\n",
            " -0.05852228 -0.08253141 -0.03526342  0.14255427 -0.02869842 -0.03413799\n",
            "  0.04708042 -0.04914371 -0.04839342 -0.09078456  0.02391535  0.02288371\n",
            "  0.03169957  0.0203515  -0.06865113  0.08178113  0.00092613 -0.02222721\n",
            "  0.0098475  -0.00550991  0.00248532 -0.02701028 -0.03226228 -0.07165227\n",
            " -0.09491113 -0.08253141 -0.00487686 -0.00928478  0.05664656  0.07765456\n",
            "  0.07352799  0.02588485  0.00886275  0.0203515  -0.00867518  0.11779484\n",
            " -0.04145328 -0.08628284 -0.04032785  0.02644757  0.02776057  0.02832328\n",
            " -0.04407928 -0.04407928  0.02260235 -0.04651771 -0.05814713  0.015756\n",
            " -0.03788942  0.02682271  0.1252977  -0.10729084 -0.04708042 -0.04951885\n",
            " -0.04951885  0.07315285  0.06602513 -0.02063285 -0.10503998 -0.03263742\n",
            " -0.04876857 -0.09266027  0.03488828 -0.05514599 -0.02701028  0.02457185\n",
            "  0.03076171 -0.07915513  0.0150995   0.01763171  0.02354021 -0.08965913\n",
            "  0.12229655 -0.0190385  -0.01313    -0.12079598 -0.06940142  0.00323561\n",
            " -0.00294253  0.03620128  0.04557985  0.01045711  0.03226228  0.02738542\n",
            " -0.08965913  0.08365684 -0.06715056  0.02119557  0.03320014 -0.05664656\n",
            " -0.01472436  0.07052685  0.08253141 -0.10804112  0.01913228 -0.07990541\n",
            " -0.00452516 -0.03076171 -0.03657642  0.03601371 -0.00661189  0.06377427\n",
            " -0.02607242  0.0701517   0.10278913  0.03094928 -0.00527545 -0.06377427\n",
            "  0.05327028 -0.07240256 -0.02944871 -0.11179256 -0.06152342  0.1252977\n",
            " -0.01650628 -0.08665799  0.01101982 -0.09678684  0.03432557  0.0780297\n",
            " -0.03601371  0.02982385  0.00284874  0.07577884  0.05627142 -0.00825314\n",
            "  0.00618986  0.13280055  0.01148875  0.06002285  0.03451314  0.07202742\n",
            "  0.02607242  0.018382   -0.01313     0.04051542  0.04764314 -0.027573\n",
            "  0.0229775  -0.01341136  0.03995271 -0.04745556  0.02869842  0.01744414\n",
            "  0.06002285  0.010504    0.00148299 -0.02091421  0.06152342  0.03244985\n",
            " -0.06302399  0.01472436 -0.07990541  0.06152342  0.00025351  0.10128856\n",
            "  0.03263742 -0.08965913 -0.04576742 -0.024947    0.01819443  0.01913228\n",
            "  0.05739685  0.00806557  0.02382157 -0.06489971  0.06602513  0.04557985\n",
            " -0.07202742  0.06452456  0.03113685 -0.04070299  0.10579026 -0.07877999\n",
            " -0.02644757 -0.04164085 -0.01716278 -0.00335284 -0.02569728 -0.06264885\n",
            " -0.04201599  0.06564999  0.06940142 -0.03976514 -0.10203884 -0.01322378\n",
            " -0.02475942  0.10879141  0.05327028  0.03995271  0.04032785 -0.04145328\n",
            "  0.13430113 -0.04670528  0.04089056 -0.04389171 -0.02907357  0.01341136\n",
            "  0.05889742  0.10128856  0.04332899 -0.03207471 -0.06002285 -0.0780297\n",
            " -0.07915513  0.00980061 -0.08703313 -0.00951925 -0.05139456  0.01885093\n",
            " -0.05101942 -0.10579026  0.00178193  0.04914371  0.04651771 -0.08815856]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('This', 1.0),\n",
              " ('It', 0.8164012432098389),\n",
              " ('That', 0.8073375225067139),\n",
              " ('The', 0.7025192379951477),\n",
              " (\"It'sa\", 0.6968192458152771),\n",
              " ('â_€_œThis', 0.6586222648620605),\n",
              " (\"That'sa\", 0.6455750465393066),\n",
              " ('Perhaps', 0.6198620796203613),\n",
              " ('More_importantly', 0.6120434999465942),\n",
              " ('Secondly', 0.6081434488296509)]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(simple_str)\n",
        "simple_str_cs = cosine_similarity(simple_str)\n",
        "print(simple_str_cs)\n",
        "wv.most_similar(simple_str_cs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moving Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def moving_avg(sentence):\n",
        "    \"\"\"\n",
        "    Define a function to calculate the\n",
        "    moving average of word embeddings\n",
        "    for tokens in a sentence\n",
        "    \"\"\"\n",
        "    toks = tokenizer(sentence)\n",
        "    avg = None\n",
        "    old = 0.7\n",
        "    new = 0.3\n",
        "    for token in toks:\n",
        "        if token.is_stop:\n",
        "            continue\n",
        "        try:\n",
        "            v = wv[token.text]\n",
        "            if avg == None:\n",
        "                avg = v\n",
        "            else:\n",
        "                avg = old * avg + v * new \n",
        "        except:\n",
        "            continue\n",
        "    return avg / np.linalg.norm(avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Moving Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.12323125 -0.03021913 -0.02109452  0.01383408 -0.09968388  0.02286058\n",
            "  0.14991827  0.04415132 -0.00642647  0.04081545  0.01040009 -0.00042618\n",
            " -0.01726807  0.07574405 -0.02070207  0.04630983  0.14128424 -0.03944185\n",
            " -0.0474872  -0.01501145  0.00431702  0.107533   -0.00603401  0.00716233\n",
            " -0.00328682 -0.02668702 -0.00726044  0.01103783 -0.01628693 -0.01147934\n",
            " -0.06514774  0.05926089  0.07378177 -0.00721138 -0.06318545  0.03787202\n",
            "  0.02148698  0.00505287  0.02668702  0.00130001  0.04905703  0.04650606\n",
            "  0.0886951  -0.02727571  0.03983431 -0.04317018 -0.06475528  0.0788837\n",
            "  0.01716996 -0.05533633 -0.0600458  -0.01025292  0.02904176 -0.08084598\n",
            " -0.01648316  0.05141176 -0.04493624 -0.107533   -0.02668702  0.00416985\n",
            " -0.00603401  0.01442277  0.01383408  0.00667176  0.04905703 -0.01599259\n",
            " -0.11852178  0.02570588  0.0474872   0.00745667  0.10203861 -0.01864167\n",
            "  0.0488608  -0.02943422 -0.11145756 -0.02727571  0.0565137   0.0726044\n",
            "  0.05533633  0.10203861 -0.05729861 -0.01824921 -0.00108539  0.04807589\n",
            "  0.06240054 -0.13657476 -0.09497441  0.00414532  0.0600458   0.00247738\n",
            " -0.01756242  0.00023149 -0.09536686 -0.08398563 -0.06318545  0.03237764\n",
            " -0.06043826 -0.0914423  -0.0565137   0.06200808 -0.00591137  0.05062685\n",
            " -0.01353974 -0.01304917 -0.07692142 -0.02158509  0.05415896 -0.05219668\n",
            "  0.04905703 -0.01211709 -0.0788837   0.02786439 -0.09183475 -0.01491334\n",
            "  0.1389295   0.09889897  0.03178895 -0.03963808 -0.01295106  0.06789493\n",
            " -0.10674809 -0.02806062 -0.0886951  -0.03689088  0.0139322   0.07142703\n",
            " -0.033555    0.05219668  0.09222721  0.07966861 -0.12087651 -0.10203861\n",
            "  0.00686798  0.0753516   0.06122317  0.03041536  0.04964571  0.00424343\n",
            "  0.0188379   0.03826448  0.00348305 -0.05415896  0.01412842  0.01726807\n",
            " -0.1326502  -0.00544533 -0.11224248 -0.07103457  0.08123843 -0.02550965\n",
            "  0.0006776  -0.02197755  0.00578873  0.00096888 -0.03590974 -0.01471711\n",
            "  0.02433228  0.01383408 -0.03316255  0.03178895 -0.07103457  0.13029547\n",
            " -0.0132454  -0.07064212  0.02119264 -0.07809879 -0.03277009 -0.07221194\n",
            " -0.05494387 -0.04944948 -0.06867984  0.0013552  -0.10282353  0.01461899\n",
            " -0.02649079  0.1318653  -0.00598496  0.01638505 -0.1012537   0.0202115\n",
            " -0.02786439 -0.05769106 -0.01824921 -0.0412079   0.0663251   0.1389295\n",
            " -0.00537174 -0.0057642   0.02904176 -0.00556797 -0.13343512  0.06475528\n",
            "  0.04532869 -0.04375887 -0.07456668 -0.01981904 -0.06946475 -0.07849124\n",
            " -0.03061159 -0.09379704  0.04297395  0.0488608  -0.0753516   0.07613651\n",
            "  0.00951706  0.03610597 -0.03120027 -0.07181948 -0.00123256 -0.04866457\n",
            " -0.05847597  0.0886951   0.01707185  0.02344926 -0.00251417 -0.00293116\n",
            "  0.05180422  0.0474872  -0.08202335  0.08751774  0.02295869  0.05141176\n",
            " -0.04925326 -0.04807589 -0.01618882 -0.03257387  0.08908756 -0.08437809\n",
            "  0.04964571  0.05062685  0.00387551  0.01520768  0.05533633  0.04160036\n",
            " -0.00419438 -0.07692142 -0.02550965  0.03806825  0.01697373  0.01913224\n",
            "  0.01648316 -0.0125586  -0.10046879 -0.06593265  0.02550965  0.04630983\n",
            " -0.01442277  0.07456668  0.00041698 -0.07692142 -0.09418949  0.06122317\n",
            "  0.1012537   0.02286058  0.05062685 -0.03237764 -0.07849124 -0.05808352\n",
            "  0.03277009 -0.06357791  0.06122317 -0.10046879 -0.04061922 -0.12166142\n",
            " -0.04101168  0.06514774  0.01628693 -0.11616704 -0.03983431 -0.02668702\n",
            "  0.08634037 -0.00858498 -0.03277009 -0.03944185  0.00156369  0.02060395\n",
            "  0.02766816 -0.08123843  0.01138123  0.03708711  0.02050584  0.02845308\n",
            " -0.01942658 -0.06318545 -0.07064212  0.04964571 -0.02550965  0.0073095\n",
            " -0.03532106  0.10596318 -0.05886843  0.05455142  0.0244304   0.02197755\n",
            " -0.0300229   0.02707948  0.10046879  0.01103783 -0.02570588  0.0565137 ]\n",
            "This is a simple string, used to test tokenization. It may have minimal information in it, but used to illustrate how to extract word embeddings\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('simple', 1.0),\n",
              " ('straightforward', 0.7460168600082397),\n",
              " ('Simple', 0.7108174562454224),\n",
              " ('uncomplicated', 0.6297484636306763),\n",
              " ('simplest', 0.6171397566795349),\n",
              " ('easy', 0.5990299582481384),\n",
              " ('fairly_straightforward', 0.5893306732177734),\n",
              " ('deceptively_simple', 0.5743065476417542),\n",
              " ('simpler', 0.5537199974060059),\n",
              " ('simplistic', 0.5516539216041565)]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_str_ma = moving_avg(simple_str)\n",
        "print(simple_str_ma)\n",
        "print(simple_str)\n",
        "wv.most_similar(simple_str_ma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Moving Average on other data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patiance is a virtue\n",
            "[('virtue', 1.0), ('dint', 0.6641983985900879), ('virture', 0.5358221530914307), ('Uncommon_valor', 0.5161334276199341), ('Imam_Mohammed_Sadiq', 0.512965202331543), ('disinterestedness', 0.5091259479522705), ('Father_Skehan', 0.49249526858329773), ('exalted', 0.4778507947921753), ('Selflessness', 0.4691542088985443), ('exemplification', 0.45130637288093567)]\n",
            "The sum of one and two is \n",
            "[('sum', 1.0000001192092896), ('sums', 0.743831992149353), ('amount', 0.6335751414299011), ('amounts', 0.5367383360862732), ('amout', 0.5301712155342102), ('outlay', 0.5228390693664551), ('GH_¢_###,###,###.##', 0.5000982880592346), ('N5million', 0.4885185956954956), ('Sums', 0.4867772161960602), ('paltry_sum', 0.47490522265434265)]\n",
            "better safe than sorry\n",
            "[('better', 1.0), ('stronger', 0.6623841524124146), ('quicker', 0.6499592065811157), ('smarter', 0.6418017148971558), ('worse', 0.6248995065689087), ('good', 0.6120728850364685), ('happier', 0.594508707523346), ('nicer', 0.590819239616394), ('easier', 0.5893970131874084), ('harder', 0.5786380767822266)]\n",
            "you cannot divide a number by zero\n",
            "[('divide', 1.0), ('divides', 0.7381654381752014), ('dividing', 0.6746436953544617), ('divided', 0.6347422003746033), ('chasm', 0.5956025719642639), ('rift', 0.5836583971977234), ('rifts', 0.5666654706001282), ('widening_gulf', 0.5594594478607178), ('widening_chasm', 0.5592597723007202), ('differences', 0.5590852499008179)]\n",
            "are we there yet?\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'NoneType'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m strings_vector:\n\u001b[1;32m      2\u001b[0m   \u001b[39mprint\u001b[39m(s)\n\u001b[0;32m----> 3\u001b[0m   \u001b[39mprint\u001b[39m(wv\u001b[39m.\u001b[39mmost_similar(moving_avg(s)))\n",
            "Cell \u001b[0;32mIn[50], line 22\u001b[0m, in \u001b[0;36mmoving_avg\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mreturn\u001b[39;00m avg \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(avg)\n",
            "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py:2526\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     sqnorm \u001b[39m=\u001b[39m x_real\u001b[39m.\u001b[39mdot(x_real) \u001b[39m+\u001b[39m x_imag\u001b[39m.\u001b[39mdot(x_imag)\n\u001b[1;32m   2525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     sqnorm \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mdot(x)\n\u001b[1;32m   2527\u001b[0m ret \u001b[39m=\u001b[39m sqrt(sqnorm)\n\u001b[1;32m   2528\u001b[0m \u001b[39mif\u001b[39;00m keepdims:\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "for s in strings_vector:\n",
        "  print(s)\n",
        "  print(wv.most_similar(moving_avg(s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Internal WV Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wv_prod(string):\n",
        "    \"\"\"Use Existing WV method\"\"\"\n",
        "    main_toks = [ t for t in tokenizer(string) ]\n",
        "    toks = []\n",
        "    for t in main_toks:\n",
        "        try:\n",
        "            v = wv[t.text]\n",
        "            if not t.is_stop:\n",
        "                toks.append(t.text)\n",
        "        except:\n",
        "            continue\n",
        "    return wv.most_similar_cosmul(positive=toks, negative=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test for simple sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Base##_encoding', 0.014345054514706135), ('fuzzing_tools', 0.01340305246412754), ('preemptive_ThreatSeeker_TM', 0.013388464227318764), ('alphabetic_characters', 0.01333235390484333), ('lnk_files', 0.01331906858831644), ('charts_graphs_maps', 0.013317907229065895), ('nonstandardized', 0.013066706247627735), ('NIST_MINEX_compliant', 0.012996304780244827), ('algebraic_formula', 0.012968824245035648), ('Javascript_extensively', 0.012934676371514797)]\n",
            "This is a simple string, used to test tokenization. It may have minimal information in it, but used to illustrate how to extract word embeddings\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('window.open', 0.745729923248291),\n",
              " ('TITLE_Debian_update', 0.7442658543586731),\n",
              " ('SOLUTION_Restrict_access', 0.7414754033088684),\n",
              " ('Display_Coleman_Liau', 0.7405574917793274),\n",
              " ('DIRECTORS_OF_CAPITAL_CORP.', 0.7392650842666626),\n",
              " ('inferential_statistics', 0.7367329597473145),\n",
              " ('xls_files', 0.727687656879425),\n",
              " ('MGRS_Military_grid', 0.7265646457672119),\n",
              " ('LINQ_queries', 0.7253888845443726),\n",
              " ('By_Miriam_Reimer', 0.7237569093704224)]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_str_wv = wv_prod(simple_str)\n",
        "print(simple_str_wv)\n",
        "print(simple_str)\n",
        "wv.most_similar(simple_str_wv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test for other sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patiance is a virtue\n",
            "[('virtue', 0.7619860172271729), ('unmerited_favor', 0.6585700511932373), ('Honesty_integrity', 0.65229731798172), ('dutifulness', 0.650239884853363), ('gentleness_kindness', 0.6296283006668091), ('dauntless_courage', 0.6238256096839905), ('omnipotent_omniscient', 0.6219653487205505), ('Graciousness', 0.6212397813796997), ('whoever_humbles', 0.6209989786148071), ('moral_uprightness', 0.6154956817626953)]\n",
            "The sum of one and two is \n",
            "[('sum', 0.8194766640663147), ('N4_###,###,###.##', 0.6124118566513062), ('N1_###,###,###', 0.5915540456771851), ('GH_¢_##,###,###.##', 0.5894641280174255), ('amountof', 0.5883204340934753), ('N##.###bn', 0.5862666964530945), ('N2_###,###.##', 0.5857111215591431), ('N4_###,###,###', 0.5802429914474487), ('N5billion', 0.5800169706344604), ('Sh##million', 0.5774497985839844)]\n",
            "better safe than sorry\n",
            "[('Said_Hirschbeck', 0.742898166179657), ('safe_Arlene_Deche', 0.7359633445739746), ('nurse_practitioner_Nunez', 0.7296922206878662), ('numb_Gwen_Bacquet', 0.7285842895507812), ('safe_Deche', 0.7275089025497437), ('guessed_Weitzman', 0.7140584588050842), ('shocked_Khubani', 0.7101832628250122), (\"Well_how're\", 0.7099193930625916), ('phenomenally_manipulative', 0.7087880969047546), ('NOVAK_DJOKOVIC_Yeah', 0.7076418399810791)]\n",
            "you cannot divide a number by zero\n",
            "[('disproportion', 0.5776875615119934), ('proportionately', 0.5637180805206299), ('divide', 0.5634515285491943), ('_Among', 0.5541926026344299), ('gini_coefficient', 0.5417084693908691), ('deciles', 0.5394157767295837), ('disparities', 0.5393581390380859), ('per_centage', 0.5377155542373657), ('Underrepresented_minorities', 0.5365667939186096), ('noncollege_educated', 0.533731997013092)]\n",
            "are we there yet?\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot compute similarity with no input",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[80], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m strings_vector:\n\u001b[1;32m      2\u001b[0m   \u001b[39mprint\u001b[39m(s)\n\u001b[0;32m----> 3\u001b[0m   \u001b[39mprint\u001b[39m(wv\u001b[39m.\u001b[39mmost_similar(wv_prod(s)))\n",
            "Cell \u001b[0;32mIn[78], line 12\u001b[0m, in \u001b[0;36mwv_prod\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mreturn\u001b[39;00m wv\u001b[39m.\u001b[39;49mmost_similar_cosmul(positive\u001b[39m=\u001b[39;49mtoks, negative\u001b[39m=\u001b[39;49m[])\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1086\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar_cosmul\u001b[0;34m(self, positive, negative, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m   1080\u001b[0m negative \u001b[39m=\u001b[39m [\n\u001b[1;32m   1081\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(word, norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(word, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m word\n\u001b[1;32m   1082\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m negative\n\u001b[1;32m   1083\u001b[0m ]\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m positive:\n\u001b[0;32m-> 1086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot compute similarity with no input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1088\u001b[0m \u001b[39m# equation (4) of Levy & Goldberg \"Linguistic Regularities...\",\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# with distances shifted to [0,1] per footnote (7)\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m pos_dists \u001b[39m=\u001b[39m [((\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m dot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectors, term) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m positive]\n",
            "\u001b[0;31mValueError\u001b[0m: cannot compute similarity with no input"
          ]
        }
      ],
      "source": [
        "for s in strings_vector:\n",
        "  print(s)\n",
        "  print(wv.most_similar(wv_prod(s)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "int method(List<string> list) \n",
            "{\n",
            "    int count = 0;\n",
            "    for (int i = 0; i < list.size(); i++) {\n",
            "        if (list.get(i).equals(\"test\")) {\n",
            "            count++;\n",
            "        }\n",
            "    }\n",
            "    return count;\n",
            "}\n",
            "\n",
            "normalized sum\n",
            "[('int', 0.7454499006271362), ('=_strlen', 0.668779194355011), ('=', 0.6647939682006836), ('=_argv', 0.6594706773757935), ('Automatic_Fuel_Injected_SILVE', 0.6416589021682739), ('len_=', 0.6287344694137573), ('strlen', 0.6204768419265747), ('==_NULL', 0.6156640648841858), ('=_null_&&', 0.6144644021987915), ('#_endif', 0.614333987236023)]\n",
            "moving average\n",
            "[('int', 1.0000001192092896), ('Bearing_Liabilities_incl.', 0.5867320895195007), ('ent', 0.5760172605514526), ('main_int_argc', 0.5722032785415649), ('ot', 0.569582462310791), ('te', 0.5633426904678345), ('nt', 0.5594090819358826), ('tr', 0.5539928674697876), ('ar', 0.5524088740348816), ('......', 0.549221932888031)]\n",
            "word2vec\n",
            "[('0_document.write', 0.8930276036262512), ('=_sizeof', 0.8857088685035706), ('0_document.write_sline', 0.8804425001144409), ('dpa_dg', 0.8802988529205322), ('dpa_si', 0.8659135699272156), ('0_&&', 0.8634930849075317), ('==_null', 0.859734058380127), ('dpa_nr', 0.8577459454536438), ('+_+_className', 0.8565471768379211), ('dpa_fm', 0.8562432527542114)]\n"
          ]
        }
      ],
      "source": [
        "# for fun experiment, test code snippets \n",
        "\n",
        "test_1 = \"\"\"\n",
        "int method(List<string> list) \n",
        "{\n",
        "    int count = 0;\n",
        "    for (int i = 0; i < list.size(); i++) {\n",
        "        if (list.get(i).equals(\"test\")) {\n",
        "            count++;\n",
        "        }\n",
        "    }\n",
        "    return count;\n",
        "}\n",
        "\"\"\"\n",
        "test_1_ns = normalized_sum(test_1)\n",
        "test_1_ma = moving_avg(test_1)\n",
        "test_1_wv = wv_prod(test_1)\n",
        "\n",
        "print(test_1)\n",
        "print('normalized sum')\n",
        "print(wv.most_similar(test_1_ns))\n",
        "print('moving average')\n",
        "print(wv.most_similar(test_1_ma))\n",
        "print('word2vec')\n",
        "print(wv.most_similar(test_1_wv))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
